diff --git a/tools/libxc/include/xenctrl.h b/tools/libxc/include/xenctrl.h
index cb4a8ddf6b..fb90cd5e66 100644
--- a/tools/libxc/include/xenctrl.h
+++ b/tools/libxc/include/xenctrl.h
@@ -1986,6 +1986,14 @@ int xc_altp2m_get_vcpu_p2m_idx(xc_interface *handle, uint32_t domid,
 int xc_altp2m_set_visibility(xc_interface *handle, uint32_t domid,
                              uint16_t view_id, bool visible);
 
+/*
+ * Operations to configure altp2m fast switching.
+ */
+int xc_altp2m_add_fast_switch(xc_interface *handle, uint32_t domid,
+		              uint32_t vcpu_id, uint64_t pgd,
+                              uint16_t view_rw, uint16_t view_x);
+int xc_altp2m_remove_fast_switch(xc_interface *handle, uint32_t domid,
+		              uint32_t vcpu_id, uint64_t pgd);
 /** 
  * Mem paging operations.
  * Paging is supported only on the x86 architecture in 64 bit mode, with
diff --git a/tools/libxc/xc_altp2m.c b/tools/libxc/xc_altp2m.c
index 6987c9541f..76ce35c628 100644
--- a/tools/libxc/xc_altp2m.c
+++ b/tools/libxc/xc_altp2m.c
@@ -434,3 +434,54 @@ int xc_altp2m_set_visibility(xc_interface *handle, uint32_t domid,
     xc_hypercall_buffer_free(handle, arg);
     return rc;
 }
+
+int xc_altp2m_add_fast_switch(xc_interface *handle, uint32_t domid,
+		              uint32_t vcpu_id, uint64_t pgd,
+			      uint16_t view_rw, uint16_t view_x)
+{
+    int rc;
+
+    DECLARE_HYPERCALL_BUFFER(xen_hvm_altp2m_op_t, arg);
+
+    arg = xc_hypercall_buffer_alloc(handle, arg, sizeof(*arg));
+    if ( arg == NULL )
+        return -1;
+
+    arg->version = HVMOP_ALTP2M_INTERFACE_VERSION;
+    arg->cmd = HVMOP_altp2m_add_fast_switch;
+    arg->domain = domid;
+    arg->u.add_fast_switch.vcpu_id = vcpu_id;
+    arg->u.add_fast_switch.pgd = pgd;
+    arg->u.add_fast_switch.view_rw = view_rw;
+    arg->u.add_fast_switch.view_x = view_x;
+
+    rc = xencall2(handle->xcall, __HYPERVISOR_hvm_op, HVMOP_altp2m,
+                  HYPERCALL_BUFFER_AS_ARG(arg));
+
+    xc_hypercall_buffer_free(handle, arg);
+    return rc;
+}
+
+int xc_altp2m_remove_fast_switch(xc_interface *handle, uint32_t domid,
+		                 uint32_t vcpu_id, uint64_t pgd)
+{
+    int rc;
+
+    DECLARE_HYPERCALL_BUFFER(xen_hvm_altp2m_op_t, arg);
+
+    arg = xc_hypercall_buffer_alloc(handle, arg, sizeof(*arg));
+    if ( arg == NULL )
+        return -1;
+
+    arg->version = HVMOP_ALTP2M_INTERFACE_VERSION;
+    arg->cmd = HVMOP_altp2m_remove_fast_switch;
+    arg->domain = domid;
+    arg->u.remove_fast_switch.vcpu_id = vcpu_id;
+    arg->u.remove_fast_switch.pgd = pgd;
+
+    rc = xencall2(handle->xcall, __HYPERVISOR_hvm_op, HVMOP_altp2m,
+                  HYPERCALL_BUFFER_AS_ARG(arg));
+
+    xc_hypercall_buffer_free(handle, arg);
+    return rc;
+}
diff --git a/xen/arch/x86/hvm/hvm.c b/xen/arch/x86/hvm/hvm.c
index 597079f8a1..faeae37cea 100644
--- a/xen/arch/x86/hvm/hvm.c
+++ b/xen/arch/x86/hvm/hvm.c
@@ -4550,6 +4550,8 @@ static int do_altp2m_op(
     case HVMOP_altp2m_change_gfn:
     case HVMOP_altp2m_get_p2m_idx:
     case HVMOP_altp2m_set_visibility:
+    case HVMOP_altp2m_add_fast_switch:
+    case HVMOP_altp2m_remove_fast_switch:
         break;
 
     default:
@@ -4841,6 +4843,34 @@ static int do_altp2m_op(
         break;
     }
 
+    case HVMOP_altp2m_add_fast_switch:
+    {
+        if ( a.u.add_fast_switch.vcpu_id >= d->max_vcpus )
+            rc = -EINVAL;
+        else if ( !altp2m_active(d) || !cpu_has_vmx_virt_exceptions )
+            rc = -EOPNOTSUPP;
+        else
+            rc = p2m_add_altp2m_fast_switch(
+                     d->vcpu[a.u.add_fast_switch.vcpu_id],
+                     a.u.add_fast_switch.pgd,
+                     a.u.add_fast_switch.view_rw,
+                     a.u.add_fast_switch.view_x);
+	break;
+    }
+
+    case HVMOP_altp2m_remove_fast_switch:
+    {
+        if ( a.u.remove_fast_switch.vcpu_id >= d->max_vcpus )
+            rc = -EINVAL;
+        else if ( !altp2m_active(d) || !cpu_has_vmx_virt_exceptions )
+            rc = -EOPNOTSUPP;
+        else
+            rc = p2m_remove_altp2m_fast_switch(
+                     d->vcpu[a.u.remove_fast_switch.vcpu_id],
+                     a.u.remove_fast_switch.pgd);
+	break;
+    }
+
     default:
         ASSERT_UNREACHABLE();
     }
diff --git a/xen/arch/x86/mm/altp2m.c b/xen/arch/x86/mm/altp2m.c
index c091b03ea3..8d7c29026c 100644
--- a/xen/arch/x86/mm/altp2m.c
+++ b/xen/arch/x86/mm/altp2m.c
@@ -27,6 +27,7 @@ altp2m_vcpu_initialise(struct vcpu *v)
         vcpu_pause(v);
 
     vcpu_altp2m(v).p2midx = 0;
+    INIT_LIST_HEAD(&vcpu_altp2m(v).fast_switch.list);
     atomic_inc(&p2m_get_altp2m(v)->active_vcpus);
 
     altp2m_vcpu_update_p2m(v);
@@ -35,6 +36,41 @@ altp2m_vcpu_initialise(struct vcpu *v)
         vcpu_unpause(v);
 }
 
+static void
+altp2m_vcpu_fast_switch_destroy(struct vcpu *v)
+{
+    struct domain *d = v->domain;
+    struct arch_domain *ad = &d->arch;
+    struct altp2mvcpu_fast_switch *fast_switch, *tmp;
+
+    /*
+     * altp2m_vcpu_destroy can be called from complete_domain_destroy even
+     * when altp2m was never initialised. In this case the fast switching
+     * list below was never created, which means there is nothing to destroy.
+     */
+    if ( !ad->altp2m_active )
+        return;
+
+    list_for_each_entry_safe ( fast_switch, tmp,
+        &vcpu_altp2m(v).fast_switch.list, list )
+    {
+        list_del(&fast_switch->list);
+        xfree(fast_switch);
+    }
+
+    /* If we are called from do_altp2m_op, we can detach on the first vCPU. */
+    if ( ad->monitor.write_ctrlreg_mandated > 0 )
+    {
+        domain_pause(d);
+        ad->monitor.write_ctrlreg_mandated = 0;
+        ad->monitor.write_ctrlreg_enabled =
+            ad->monitor.write_ctrlreg_requested;
+        for_each_vcpu ( d, v )
+            hvm_update_guest_cr(v, 0);
+        domain_unpause(d);
+    }
+}
+
 void
 altp2m_vcpu_destroy(struct vcpu *v)
 {
@@ -46,6 +82,7 @@ altp2m_vcpu_destroy(struct vcpu *v)
     if ( (p2m = p2m_get_altp2m(v)) )
         atomic_dec(&p2m->active_vcpus);
 
+    altp2m_vcpu_fast_switch_destroy(v);
     altp2m_vcpu_disable_ve(v);
 
     vcpu_altp2m(v).p2midx = INVALID_ALTP2M;
diff --git a/xen/arch/x86/mm/p2m.c b/xen/arch/x86/mm/p2m.c
index db7bde0230..2c9befc80e 100644
--- a/xen/arch/x86/mm/p2m.c
+++ b/xen/arch/x86/mm/p2m.c
@@ -38,6 +38,7 @@
 #include <asm/hvm/nestedhvm.h>
 #include <asm/altp2m.h>
 #include <asm/vm_event.h>
+#include <asm/monitor.h>
 #include <xsm/xsm.h>
 
 #include "mm-locks.h"
@@ -2094,6 +2095,108 @@ bool p2m_altp2m_get_or_propagate(struct p2m_domain *ap2m, unsigned long gfn_l,
     return true;
 }
 
+static bool p2m_altp2m_get_fast_switch_by_cr3(struct vcpu *v,
+                 struct altp2mvcpu_fast_switch **out,
+		 unsigned long cr3)
+{
+    struct altp2mvcpu_fast_switch *fast_switch, *fallback = NULL;
+    unsigned long pgd = cr3;
+
+    if ( !altp2m_active(v->domain) )
+        return false;
+
+    /* Mask off KPTI bits. */
+    if ( hvm_pcid_enabled(v) )
+        pgd &= ~0x1FFF;
+
+    list_for_each_entry ( fast_switch, &vcpu_altp2m(v).fast_switch.list, list )
+    {
+	if ( fast_switch->pgd == pgd )
+        {
+            *out = fast_switch;
+            return true;
+        }
+
+	if ( !fast_switch->pgd )
+		fallback = fast_switch;
+    }
+
+    if ( fallback )
+        *out = fallback;
+
+    return fallback != NULL;
+}
+
+
+bool p2m_altp2m_perform_fast_switch(struct vcpu *v, vm_event_request_t *req)
+{
+    struct domain *d = v->domain;
+    struct arch_domain *ad = &d->arch;
+    uint16_t p2midx = vcpu_altp2m(v).p2midx;
+    struct altp2mvcpu_fast_switch *fast_switch = NULL,
+                                  *prev_fast_switch = NULL;
+    bool unrequested = false;
+
+    if ( req->reason == VM_EVENT_REASON_WRITE_CTRLREG &&
+         req->u.write_ctrlreg.index == VM_EVENT_X86_CR3 )
+    {
+        /* Look up previous and current fast switch configurations. */
+        p2m_altp2m_get_fast_switch_by_cr3(v, &fast_switch,
+                                          req->u.write_ctrlreg.new_value);
+        p2m_altp2m_get_fast_switch_by_cr3(v, &prev_fast_switch,
+                                          req->u.write_ctrlreg.old_value);
+
+        /*
+	 * It usually doesn't matter which view we pick here, but most
+	 * modifications will be on code pages, so by going directly
+	 * to the execute view we can avoid the additional violation.
+	 */
+	if ( fast_switch )
+            p2midx = fast_switch->p2midx_x;
+	/* Switch to the default view when fast switching is disabled. */
+	else if ( prev_fast_switch )
+            p2midx = 0;
+
+        /* If the event was not requested, we can just drop it here. */
+        if ( !(ad->monitor.write_ctrlreg_requested &
+               monitor_ctrlreg_bitmask(VM_EVENT_X86_CR3)) )
+            unrequested = true;
+    }
+
+    /*
+     * Perform short-circuit altp2m state transition.
+     * This avoids excessive context-switches to the monitor.
+     */
+    if ( req->reason == VM_EVENT_REASON_MEM_ACCESS &&
+         p2m_altp2m_get_fast_switch_by_cr3(v, &fast_switch,
+				              v->arch.hvm.guest_cr[3]) )
+    {
+	if ( (req->u.mem_access.flags & MEM_ACCESS_X) &&
+             p2midx == fast_switch->p2midx_rw )
+	{
+             p2midx = fast_switch->p2midx_x;
+             unrequested = true;
+        }
+
+	if ( (req->u.mem_access.flags & MEM_ACCESS_RW) &&
+             p2midx == fast_switch->p2midx_x )
+        {
+             p2midx = fast_switch->p2midx_rw;
+             unrequested = true;
+        }
+    }
+
+    /* Switch altp2m view and resume execution. */
+    if ( p2midx != vcpu_altp2m(v).p2midx )
+    {
+        vcpu_pause_nosync(v);
+        p2m_switch_vcpu_altp2m_by_id(v, p2midx);
+        vcpu_unpause(v);
+    }
+
+    return unrequested;
+}
+
 enum altp2m_reset_type {
     ALTP2M_RESET,
     ALTP2M_DEACTIVATE
@@ -2794,6 +2897,108 @@ int p2m_set_altp2m_view_visibility(struct domain *d, unsigned int altp2m_idx,
 
     return rc;
 }
+
+static void p2m_set_write_ctrlreg_for_cr3(struct domain *d, bool value)
+{
+    struct vcpu *v;
+    struct arch_domain *ad = &d->arch;
+    unsigned int mask = ad->monitor.write_ctrlreg_mandated;
+
+    if ( value )
+        mask |= monitor_ctrlreg_bitmask(VM_EVENT_X86_CR3);
+    else
+        mask &= ~monitor_ctrlreg_bitmask(VM_EVENT_X86_CR3);
+
+    if ( ad->monitor.write_ctrlreg_mandated == mask )
+        return;
+
+    ad->monitor.write_ctrlreg_mandated = mask;
+    ad->monitor.write_ctrlreg_enabled =
+        ad->monitor.write_ctrlreg_requested |
+        ad->monitor.write_ctrlreg_mandated;
+
+    for_each_vcpu ( d, v )
+        hvm_update_guest_cr(v, 0);
+}
+
+int p2m_add_altp2m_fast_switch(struct vcpu *v, unsigned long pgd,
+                               unsigned int view_rw, unsigned int view_x)
+{
+    int rc = 0;
+    struct domain *d = v->domain;
+    struct altp2mvcpu_fast_switch *fast_switch;
+
+    if ( unlikely(view_rw == view_x || view_rw <= 0 || view_x <= 0 ||
+                  view_rw >= MAX_ALTP2M || view_x >= MAX_ALTP2M) )
+        return -EINVAL;
+
+    rc = domain_pause_except_self(d);
+    if ( rc )
+        return rc;
+
+    altp2m_list_lock(d);
+
+    /* Prevent insertion of duplicates. */
+    list_for_each_entry ( fast_switch, &vcpu_altp2m(v).fast_switch.list, list )
+        if ( fast_switch->pgd == pgd )
+        {
+            altp2m_list_unlock(d);
+	    domain_unpause_except_self(d);
+            return -EPERM;
+        }
+
+    fast_switch = xmalloc(struct altp2mvcpu_fast_switch);
+    fast_switch->pgd = pgd;
+    fast_switch->p2midx_rw = view_rw;
+    fast_switch->p2midx_x = view_x;
+    list_add(&fast_switch->list, &vcpu_altp2m(v).fast_switch.list);
+
+    p2m_set_write_ctrlreg_for_cr3(d, true);
+    altp2m_list_unlock(d);
+
+    domain_unpause_except_self(d);
+
+    return rc;
+}
+
+
+int p2m_remove_altp2m_fast_switch(struct vcpu *v, unsigned long pgd)
+{
+    int rc = -EINVAL;
+    struct domain *d = v->domain;
+    struct list_head *it, *tmp;
+    struct altp2mvcpu_fast_switch *fast_switch;
+    bool remaining = false;
+
+    rc = domain_pause_except_self(d);
+    if ( rc )
+        return rc;
+
+    altp2m_list_lock(d);
+
+    list_for_each_safe ( it, tmp, &vcpu_altp2m(v).fast_switch.list )
+    {
+        fast_switch = list_entry( it, struct altp2mvcpu_fast_switch, list );
+
+        if ( fast_switch->pgd == pgd )
+        {
+            list_del(it);
+            xfree(fast_switch);
+            rc = 0;
+        }
+	else
+            remaining = true;
+    }
+
+    if ( !rc && !remaining )
+        p2m_set_write_ctrlreg_for_cr3(d, false);
+
+    altp2m_list_unlock(d);
+
+    domain_unpause_except_self(d);
+
+    return rc;
+}
 #endif
 
 /*
diff --git a/xen/arch/x86/monitor.c b/xen/arch/x86/monitor.c
index 6f04fe38b7..f2b137f480 100644
--- a/xen/arch/x86/monitor.c
+++ b/xen/arch/x86/monitor.c
@@ -236,7 +236,7 @@ int arch_monitor_domctl_event(struct domain *d,
             return -EINVAL;
 
         ctrlreg_bitmask = monitor_ctrlreg_bitmask(mop->u.mov_to_cr.index);
-        old_status = !!(ad->monitor.write_ctrlreg_enabled & ctrlreg_bitmask);
+        old_status = !!(ad->monitor.write_ctrlreg_requested & ctrlreg_bitmask);
 
         if ( unlikely(old_status == requested_status) )
             return -EEXIST;
@@ -256,14 +256,17 @@ int arch_monitor_domctl_event(struct domain *d,
         if ( requested_status )
         {
             ad->monitor.write_ctrlreg_mask[mop->u.mov_to_cr.index] = mop->u.mov_to_cr.bitmask;
-            ad->monitor.write_ctrlreg_enabled |= ctrlreg_bitmask;
+            ad->monitor.write_ctrlreg_requested |= ctrlreg_bitmask;
         }
         else
         {
             ad->monitor.write_ctrlreg_mask[mop->u.mov_to_cr.index] = 0;
-            ad->monitor.write_ctrlreg_enabled &= ~ctrlreg_bitmask;
+            ad->monitor.write_ctrlreg_requested &= ~ctrlreg_bitmask;
         }
 
+        ad->monitor.write_ctrlreg_enabled = ad->monitor.write_ctrlreg_mandated |
+                                            ad->monitor.write_ctrlreg_requested;
+
         if ( VM_EVENT_X86_CR3 == mop->u.mov_to_cr.index ||
              VM_EVENT_X86_CR4 == mop->u.mov_to_cr.index )
         {
diff --git a/xen/common/monitor.c b/xen/common/monitor.c
index d5c9ff1cbf..aa4ef74ea5 100644
--- a/xen/common/monitor.c
+++ b/xen/common/monitor.c
@@ -26,6 +26,7 @@
 #include <xsm/xsm.h>
 #include <asm/altp2m.h>
 #include <asm/monitor.h>
+#include <asm/p2m.h>
 #include <asm/vm_event.h>
 
 int monitor_domctl(struct domain *d, struct xen_domctl_monitor_op *mop)
@@ -93,6 +94,11 @@ int monitor_traps(struct vcpu *v, bool sync, vm_event_request_t *req)
     int rc;
     struct domain *d = v->domain;
 
+#ifdef CONFIG_HVM
+    if ( p2m_altp2m_perform_fast_switch(v, req) )
+        return 1;
+#endif
+
     rc = vm_event_claim_slot(d, d->vm_event_monitor);
     switch ( rc )
     {
diff --git a/xen/include/asm-arm/p2m.h b/xen/include/asm-arm/p2m.h
index 5fdb6e8183..e12a0213e5 100644
--- a/xen/include/asm-arm/p2m.h
+++ b/xen/include/asm-arm/p2m.h
@@ -156,6 +156,12 @@ void p2m_altp2m_check(struct vcpu *v, uint16_t idx)
     /* Not supported on ARM. */
 }
 
+static inline
+void p2m_altp2m_perform_fast_switch(struct vcpu *v, vm_event_request_t *req)
+{
+    /* Not supported on ARM. */
+}
+
 /*
  * Helper to restrict "p2m_ipa_bits" according the external entity
  * (e.g. IOMMU) requirements.
diff --git a/xen/include/asm-x86/domain.h b/xen/include/asm-x86/domain.h
index baf1f1da54..f029f994da 100644
--- a/xen/include/asm-x86/domain.h
+++ b/xen/include/asm-x86/domain.h
@@ -414,6 +414,8 @@ struct arch_domain
     /* Arch-specific monitor options */
     struct {
         unsigned int write_ctrlreg_enabled                                 : 4;
+        unsigned int write_ctrlreg_mandated                                : 4;
+        unsigned int write_ctrlreg_requested                               : 4;
         unsigned int write_ctrlreg_sync                                    : 4;
         unsigned int write_ctrlreg_onchangeonly                            : 4;
         unsigned int singlestep_enabled                                    : 1;
diff --git a/xen/include/asm-x86/hvm/vcpu.h b/xen/include/asm-x86/hvm/vcpu.h
index 5ccd075815..2096b4918c 100644
--- a/xen/include/asm-x86/hvm/vcpu.h
+++ b/xen/include/asm-x86/hvm/vcpu.h
@@ -137,6 +137,13 @@ struct nestedvcpu {
 
 #define vcpu_nestedhvm(v) ((v)->arch.hvm.nvcpu)
 
+struct altp2mvcpu_fast_switch {
+    uint64_t pgd;
+    uint16_t p2midx_rw;
+    uint16_t p2midx_x;
+    struct list_head list;
+};
+
 struct altp2mvcpu {
     /*
      * #VE information page.  This pointer being non-NULL indicates that a
@@ -145,6 +152,8 @@ struct altp2mvcpu {
      */
     struct page_info *veinfo_pg;
     uint16_t    p2midx;         /* alternate p2m index */
+    /* list of all active fast switch mappings for this vcpu */
+    struct altp2mvcpu_fast_switch fast_switch;
 };
 
 #define vcpu_altp2m(v) ((v)->arch.hvm.avcpu)
diff --git a/xen/include/asm-x86/p2m.h b/xen/include/asm-x86/p2m.h
index 8abae345e8..b24a86a7b6 100644
--- a/xen/include/asm-x86/p2m.h
+++ b/xen/include/asm-x86/p2m.h
@@ -30,6 +30,7 @@
 #include <xen/mem_access.h>
 #include <asm/mem_sharing.h>
 #include <asm/page.h>    /* for pagetable_t */
+#include <public/vm_event.h>
 
 extern bool_t opt_hap_1gb, opt_hap_2mb;
 
@@ -870,6 +871,9 @@ bool p2m_altp2m_get_or_propagate(struct p2m_domain *ap2m, unsigned long gfn_l,
                                  mfn_t *mfn, p2m_type_t *p2mt,
                                  p2m_access_t *p2ma, unsigned int page_order);
 
+/* Apply altp2m fast switching when suitable */
+bool p2m_altp2m_perform_fast_switch(struct vcpu *v, vm_event_request_t *req);
+
 /* Make a specific alternate p2m valid */
 int p2m_init_altp2m_by_id(struct domain *d, unsigned int idx);
 
@@ -895,6 +899,11 @@ int p2m_altp2m_propagate_change(struct domain *d, gfn_t gfn,
 /* Set a specific p2m view visibility */
 int p2m_set_altp2m_view_visibility(struct domain *d, unsigned int idx,
                                    uint8_t visible);
+
+/* Operations to configure altp2m fast switching */
+int p2m_add_altp2m_fast_switch(struct vcpu *v, unsigned long pgd,
+                               unsigned int view_rw, unsigned int view_x);
+int p2m_remove_altp2m_fast_switch(struct vcpu *v, unsigned long pgd);
 #else
 struct p2m_domain *p2m_get_altp2m(struct vcpu *v);
 static inline void p2m_altp2m_check(struct vcpu *v, uint16_t idx) {}
diff --git a/xen/include/public/hvm/hvm_op.h b/xen/include/public/hvm/hvm_op.h
index 870ec52060..7eae6cdbda 100644
--- a/xen/include/public/hvm/hvm_op.h
+++ b/xen/include/public/hvm/hvm_op.h
@@ -324,6 +324,18 @@ struct xen_hvm_altp2m_set_visibility {
     uint8_t pad;
 };
 
+struct xen_hvm_altp2m_add_fast_switch {
+    uint32_t vcpu_id;
+    uint64_t pgd;
+    uint16_t view_rw;
+    uint16_t view_x;
+};
+
+struct xen_hvm_altp2m_remove_fast_switch {
+    uint32_t vcpu_id;
+    uint64_t pgd;
+};
+
 struct xen_hvm_altp2m_op {
     uint32_t version;   /* HVMOP_ALTP2M_INTERFACE_VERSION */
     uint32_t cmd;
@@ -358,6 +370,10 @@ struct xen_hvm_altp2m_op {
 #define HVMOP_altp2m_set_suppress_ve_multi 15
 /* Set visibility for a given altp2m view */
 #define HVMOP_altp2m_set_visibility       16
+/* Adds a preconfigured altp2m fast switch */
+#define HVMOP_altp2m_add_fast_switch      17
+/* Removes a preconfigured altp2m fast switch */
+#define HVMOP_altp2m_remove_fast_switch   18
     domid_t domain;
     uint16_t pad1;
     uint32_t pad2;
@@ -376,6 +392,8 @@ struct xen_hvm_altp2m_op {
         struct xen_hvm_altp2m_vcpu_disable_notify  disable_notify;
         struct xen_hvm_altp2m_get_vcpu_p2m_idx     get_vcpu_p2m_idx;
         struct xen_hvm_altp2m_set_visibility       set_visibility;
+        struct xen_hvm_altp2m_add_fast_switch      add_fast_switch;
+        struct xen_hvm_altp2m_remove_fast_switch   remove_fast_switch;
         uint8_t pad[64];
     } u;
 };
